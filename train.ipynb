{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrdtSKPRCBw8E17R5c2d0t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryanmaxwell96/trpo_fractal5NN/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6TMPg8YrBiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! /usr/bin/env python3\n",
        "\"\"\"\n",
        "TRPO: Trust Region Policy Optimization\n",
        "\n",
        "Written by Patrick Coady (pat-coady.github.io)\n",
        "\n",
        "See these papers for details:\n",
        "\n",
        "TRPO / PPO:\n",
        "https://arxiv.org/pdf/1502.05477.pdf (Schulman et al., 2016)\n",
        "\n",
        "Distributed PPO:\n",
        "https://arxiv.org/abs/1707.02286 (Heess et al., 2017)\n",
        "\n",
        "Generalized Advantage Estimation:\n",
        "https://arxiv.org/pdf/1506.02438.pdf\n",
        "\n",
        "And, also, this GitHub repo which was helpful to me during\n",
        "implementation:\n",
        "https://github.com/joschu/modular_rl\n",
        "\n",
        "This implementation learns policies for continuous environments\n",
        "in the OpenAI Gym (https://gym.openai.com/). Testing was focused on\n",
        "the MuJoCo control tasks.\n",
        "\"\"\"\n",
        "import gym\n",
        "import pybullet\n",
        "import pybullet_envs\n",
        "import numpy as np\n",
        "from gym import wrappers\n",
        "from policy import Policy\n",
        "from value import NNValueFunction\n",
        "import scipy.signal\n",
        "from utils import Logger, Scaler\n",
        "from datetime import datetime\n",
        "import os\n",
        "import argparse\n",
        "import signal\n",
        "\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "import pickle\n",
        "import tempfile\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "class GracefulKiller:\n",
        "    \"\"\"Gracefully exit program on CTRL-C.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.kill_now = False\n",
        "        signal.signal(signal.SIGINT, self.exit_gracefully)\n",
        "        signal.signal(signal.SIGTERM, self.exit_gracefully)\n",
        "\n",
        "    def exit_gracefully(self, signum, frame):\n",
        "        self.kill_now = True\n",
        "\n",
        "\n",
        "def init_gym(env_name):\n",
        "    \"\"\"\n",
        "    Initialize gym environment, return dimension of observation\n",
        "    and action spaces.\n",
        "\n",
        "    Args:\n",
        "        env_name: str environment name (e.g. \"Humanoid-v1\")\n",
        "\n",
        "    Returns: 3-tuple\n",
        "        gym environment (object)\n",
        "        number of observation dimensions (int)\n",
        "        number of action dimensions (int)\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    if env_name == 'CartPoleBulletEnv-v1':\n",
        "        act_dim = 1\n",
        "        obs_dim = 4\n",
        "        # states: \n",
        "        # theta -  x>0 when tilts right (CW)\n",
        "        # x_dot - x<0 moving left\n",
        "    else:\n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        act_dim = env.action_space.shape[0]\n",
        "\n",
        "    return env, obs_dim, act_dim\n",
        "\n",
        "\n",
        "def run_episode(env, policy, scaler, animate=False):\n",
        "    \"\"\"Run single episode with option to animate.\n",
        "\n",
        "    Args:\n",
        "        env: ai gym environment\n",
        "        policy: policy object with sample() method\n",
        "        scaler: scaler object, used to scale/offset each observation dimension\n",
        "            to a similar range\n",
        "        animate: boolean, True uses env.render() method to animate episode\n",
        "\n",
        "    Returns: 4-tuple of NumPy arrays\n",
        "        observes: shape = (episode len, obs_dim)\n",
        "        actions: shape = (episode len, act_dim)\n",
        "        rewards: shape = (episode len,)\n",
        "        unscaled_obs: useful for training scaler, shape = (episode len, obs_dim)\n",
        "    \"\"\"\n",
        "    obs = env.reset()\n",
        "    observes, actions, rewards, unscaled_obs = [], [], [], []\n",
        "    done = False\n",
        "    step = 0.0\n",
        "    scale, offset = scaler.get()\n",
        "    scale[-1] = 1.0  # don't scale time step feature\n",
        "    offset[-1] = 0.0  # don't offset time step feature\n",
        "    while not done:\n",
        "        if animate:\n",
        "            env.render()\n",
        "        obs = np.concatenate([obs, [step]])  # add time step feature\n",
        "        obs = obs.astype(np.float32).reshape((1, -1))\n",
        "        unscaled_obs.append(obs)\n",
        "        obs = np.float32((obs - offset) * scale)  # center and scale observations\n",
        "        observes.append(obs)\n",
        "        action = policy.sample(obs)\n",
        "\n",
        "        # print('action')\n",
        "        # print(action)\n",
        "        # input('')\n",
        "        actions.append(action)\n",
        "        obs, reward, done, _ = env.step(action.flatten()) #\n",
        "        rewards.append(reward)\n",
        "        step += 1e-3  # increment time step feature\n",
        "\n",
        "    return (np.concatenate(observes), np.concatenate(actions),\n",
        "            np.array(rewards, dtype=np.float32), np.concatenate(unscaled_obs))\n",
        "\n",
        "\n",
        "def run_policy(env, policy, scaler, logger, episodes):\n",
        "    \"\"\" Run policy and collect data for a minimum of min_steps and min_episodes\n",
        "\n",
        "    Args:\n",
        "        env: ai gym environment\n",
        "        policy: policy object with sample() method\n",
        "        scaler: scaler object, used to scale/offset each observation dimension\n",
        "            to a similar range\n",
        "        logger: logger object, used to save stats from episodes\n",
        "        episodes: total episodes to run\n",
        "\n",
        "    Returns: list of trajectory dictionaries, list length = number of episodes\n",
        "        'observes' : NumPy array of states from episode\n",
        "        'actions' : NumPy array of actions from episode\n",
        "        'rewards' : NumPy array of (un-discounted) rewards from episode\n",
        "        'unscaled_obs' : NumPy array of (un-discounted) rewards from episode\n",
        "    \"\"\"\n",
        "    total_steps = 0\n",
        "    trajectories = []\n",
        "    for e in range(episodes):\n",
        "        observes, actions, rewards, unscaled_obs = run_episode(env, policy, scaler)\n",
        "        # print(observes.shape)\n",
        "        # print(actions.shape)\n",
        "        # print(rewards.shape)\n",
        "        # print(unscaled_obs.shape)\n",
        "        # print(observes.dtype)\n",
        "        # print(actions.dtype)\n",
        "        # print(rewards.dtype)\n",
        "        # print(unscaled_obs.dtype)\n",
        "        total_steps += observes.shape[0]\n",
        "        trajectory = {'observes': observes,\n",
        "                      'actions': actions,\n",
        "                      'rewards': rewards,\n",
        "                      'unscaled_obs': unscaled_obs}\n",
        "        trajectories.append(trajectory)\n",
        "    unscaled = np.concatenate([t['unscaled_obs'] for t in trajectories])\n",
        "    scaler.update(unscaled)  # update running statistics for scaling observations\n",
        "    logger.log({'_MeanReward': np.mean([t['rewards'].sum() for t in trajectories]),\n",
        "                'Steps': total_steps})\n",
        "\n",
        "    return trajectories\n",
        "\n",
        "\n",
        "def discount(x, gamma):\n",
        "    \"\"\" Calculate discounted forward sum of a sequence at each point \"\"\"\n",
        "    return scipy.signal.lfilter([1.0], [1.0, -gamma], x[::-1])[::-1]\n",
        "\n",
        "\n",
        "def add_disc_sum_rew(trajectories, gamma):\n",
        "    \"\"\" Adds discounted sum of rewards to all time steps of all trajectories\n",
        "\n",
        "    Args:\n",
        "        trajectories: as returned by run_policy()\n",
        "        gamma: discount\n",
        "\n",
        "    Returns:\n",
        "        None (mutates trajectories dictionary to add 'disc_sum_rew')\n",
        "    \"\"\"\n",
        "    for trajectory in trajectories:\n",
        "        if gamma < 0.999:  # don't scale for gamma ~= 1\n",
        "            rewards = trajectory['rewards'] * (1 - gamma)\n",
        "        else:\n",
        "            rewards = trajectory['rewards']\n",
        "        disc_sum_rew = discount(rewards, gamma)\n",
        "        trajectory['disc_sum_rew'] = disc_sum_rew\n",
        "\n",
        "\n",
        "def add_value(trajectories, val_func):\n",
        "    \"\"\" Adds estimated value to all time steps of all trajectories\n",
        "\n",
        "    Args:\n",
        "        trajectories: as returned by run_policy()\n",
        "        val_func: object with predict() method, takes observations\n",
        "            and returns predicted state value\n",
        "\n",
        "    Returns:\n",
        "        None (mutates trajectories dictionary to add 'values')\n",
        "    \"\"\"\n",
        "    for trajectory in trajectories:\n",
        "        observes = trajectory['observes']\n",
        "        values = val_func.predict(observes)\n",
        "        trajectory['values'] = values.flatten()\n",
        "\n",
        "\n",
        "def add_gae(trajectories, gamma, lam):\n",
        "    \"\"\" Add generalized advantage estimator.\n",
        "    https://arxiv.org/pdf/1506.02438.pdf\n",
        "\n",
        "    Args:\n",
        "        trajectories: as returned by run_policy(), must include 'values'\n",
        "            key from add_value().\n",
        "        gamma: reward discount\n",
        "        lam: lambda (see paper).\n",
        "            lam=0 : use TD residuals\n",
        "            lam=1 : A =  Sum Discounted Rewards - V_hat(s)\n",
        "\n",
        "    Returns:\n",
        "        None (mutates trajectories dictionary to add 'advantages')\n",
        "    \"\"\"\n",
        "    for trajectory in trajectories:\n",
        "        if gamma < 0.999:  # don't scale for gamma ~= 1\n",
        "            rewards = trajectory['rewards'] * (1 - gamma) # discount rewards\n",
        "        else:\n",
        "            rewards = trajectory['rewards']\n",
        "        values = trajectory['values']\n",
        "        # temporal differences\n",
        "        tds = rewards - values + np.append(values[1:] * gamma, 0)\n",
        "        advantages = discount(tds, gamma * lam)\n",
        "        trajectory['advantages'] = advantages\n",
        "\n",
        "\n",
        "def build_train_set(trajectories):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        trajectories: trajectories after processing by add_disc_sum_rew(),\n",
        "            add_value(), and add_gae()\n",
        "\n",
        "    Returns: 4-tuple of NumPy arrays\n",
        "        observes: shape = (N, obs_dim)\n",
        "        actions: shape = (N, act_dim)\n",
        "        advantages: shape = (N,)\n",
        "        disc_sum_rew: shape = (N,)\n",
        "    \"\"\"\n",
        "    observes = np.concatenate([t['observes'] for t in trajectories])\n",
        "    actions = np.concatenate([t['actions'] for t in trajectories])\n",
        "    disc_sum_rew = np.concatenate([t['disc_sum_rew'] for t in trajectories])\n",
        "    advantages = np.concatenate([t['advantages'] for t in trajectories])\n",
        "    # normalize advantages\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)\n",
        "\n",
        "    return observes, actions, advantages, disc_sum_rew\n",
        "\n",
        "\n",
        "def log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode):\n",
        "    \"\"\" Log various batch statistics \"\"\"\n",
        "    logger.log({'_mean_obs': np.mean(observes),\n",
        "                '_min_obs': np.min(observes),\n",
        "                '_max_obs': np.max(observes),\n",
        "                '_std_obs': np.mean(np.var(observes, axis=0)),\n",
        "                '_mean_act': np.mean(actions),\n",
        "                '_min_act': np.min(actions),\n",
        "                '_max_act': np.max(actions),\n",
        "                '_std_act': np.mean(np.var(actions, axis=0)),\n",
        "                '_mean_adv': np.mean(advantages),\n",
        "                '_min_adv': np.min(advantages),\n",
        "                '_max_adv': np.max(advantages),\n",
        "                '_std_adv': np.var(advantages),\n",
        "                '_mean_discrew': np.mean(disc_sum_rew),\n",
        "                '_min_discrew': np.min(disc_sum_rew),\n",
        "                '_max_discrew': np.max(disc_sum_rew),\n",
        "                '_std_discrew': np.var(disc_sum_rew),\n",
        "                '_Episode': episode\n",
        "                })\n",
        "\n",
        "\n",
        "def main(env_name, num_episodes, gamma, lam, kl_targ, batch_size, hid1_mult, init_logvar):\n",
        "    \"\"\" Main training loop\n",
        "\n",
        "    Args:\n",
        "        env_name: OpenAI Gym environment name, e.g. 'Hopper-v1'\n",
        "        num_episodes: maximum number of episodes to run\n",
        "        gamma: reward discount factor (float)\n",
        "        lam: lambda from Generalized Advantage Estimate\n",
        "        kl_targ: D_KL target for policy update [D_KL(pi_old || pi_new)\n",
        "        batch_size: number of episodes per policy training batch\n",
        "        hid1_mult: hid1 size for policy and value_f (multiplier of obs dimension)\n",
        "        init_logvar: natural log of initial policy variance\n",
        "    \"\"\"\n",
        "    print('load model (l)?')\n",
        "    loading = input('')\n",
        "    pybullet.connect(pybullet.DIRECT)\n",
        "    killer = GracefulKiller()\n",
        "    env, obs_dim, act_dim = init_gym(env_name)\n",
        "    obs_dim += 1  # add 1 to obs dimension for time step feature (see run_episode())\n",
        "    # print('obs_dim') # 45 for HumanoidFlagrunBulletEnv-v0, HumanoidFlagrunHarderBulletEnv-v0\n",
        "    # print(obs_dim)\n",
        "    # print('act_dim') # 17 for HumanoidFlagrunBelletEnv-v0, HumanoidFlagrunHarderBulletEnv-v0\n",
        "    # print(act_dim)\n",
        "    # input('')\n",
        "    now = datetime.utcnow().strftime(\"%b-%d_%H:%M:%S\")  # create unique directories\n",
        "    logger = Logger(logname=env_name, now=now)\n",
        "    aigym_path = os.path.join('/tmp', env_name, now)\n",
        "    env = wrappers.Monitor(env, aigym_path, force=True)\n",
        "    scaler = Scaler(obs_dim)\n",
        "\n",
        "    val_func = NNValueFunction(obs_dim, hid1_mult, loading)\n",
        "    policy = Policy(obs_dim, act_dim, kl_targ, hid1_mult, init_logvar)\n",
        "    # run a few episodes of untrained policy to initialize scaler:\n",
        "    run_policy(env, policy, scaler, logger, episodes=5)\n",
        "\n",
        "    policy_model = policy.get_trpo_policy_model()\n",
        "    valNN_model = val_func.get_valNN_model()\n",
        "    lr = val_func.get_lr()\n",
        "\n",
        "    if loading == 'l':\n",
        "        policy_model.load_weights('pol_weights.h5')\n",
        "        pol_weights = policy_model.get_weights()\n",
        "        print('pol_weights')\n",
        "        print(pol_weights)\n",
        "        input('')\n",
        "        loading == 'n'\n",
        "\n",
        "    save_weights_flag = 1\n",
        "    save_model = 1\n",
        "\n",
        "    episode = 0\n",
        "    while episode < num_episodes:\n",
        "        trajectories = run_policy(env, policy, scaler, logger, episodes=batch_size)\n",
        "        episode += len(trajectories)\n",
        "\n",
        "        if episode <= batch_size:\n",
        "            if loading == 'l':\n",
        "                traj = open('trajectories.obj', 'rb')\n",
        "                trajectories = pickle.load(traj)\n",
        "                traj.close()\n",
        "                print('342')\n",
        "                input('')\n",
        "        elif episode == num_episodes-batch_size:\n",
        "            traj = open('trajectories.obj','wb')\n",
        "            pickle.dump(trajectories,traj)\n",
        "            traj.close()\n",
        "            print('348')\n",
        "            input('')\n",
        "\n",
        "        add_value(trajectories, val_func)  # add estimated values to episodes\n",
        "        add_disc_sum_rew(trajectories, gamma)  # calculated discounted sum of Rs\n",
        "        add_gae(trajectories, gamma, lam)  # calculate advantage\n",
        "        # concatenate all episodes into single NumPy arrays\n",
        "        observes, actions, advantages, disc_sum_rew = build_train_set(trajectories)\n",
        "        # add various stats to training log:\n",
        "        log_batch_stats(observes, actions, advantages, disc_sum_rew, logger, episode)\n",
        "        policy.update(observes, actions, advantages, logger)  # update policy\n",
        "        if save_model == 1:\n",
        "            policy_model = policy.get_trpo_policy_model()\n",
        "            policy_model.save( 'policy_model')\n",
        "            save_model = 0\n",
        "        val_func.fit(observes, disc_sum_rew, logger)  # update value function\n",
        "        logger.write(display=True)  # write logger results to file and stdout\n",
        "        if killer.kill_now:\n",
        "            if input('Terminate training (y/[n])? ') == 'y':\n",
        "                break\n",
        "            killer.kill_now = False\n",
        "    logger.close()\n",
        "\n",
        "    if save_weights_flag == 1:\n",
        "        valNN_model.save('val_weights.h5')\n",
        "\n",
        "        policy_weights = policy_model.get_weights()\n",
        "        print('policy_weights')\n",
        "        print(policy_weights)\n",
        "        input('')\n",
        "        # policy_model.save_weights('pol_weights.hdf5')\n",
        "        policy_model.save_weights('pol_weights.h5')\n",
        "        # tf.keras.models.save_model(policy_model,'~/trpo_fractal5NN/trpo')\n",
        "        # save_weights_flag = 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=('Train policy on OpenAI Gym environment '\n",
        "                                                  'using Proximal Policy Optimizer'))\n",
        "    parser.add_argument('env_name', type=str, help='OpenAI Gym (PyBullet) environment name')\n",
        "    parser.add_argument('-n', '--num_episodes', type=int, help='Number of episodes to run',\n",
        "                        default=500)\n",
        "    parser.add_argument('-g', '--gamma', type=float, help='Discount factor', default=0.995)\n",
        "    parser.add_argument('-l', '--lam', type=float, help='Lambda for Generalized Advantage Estimation',\n",
        "                        default=0.98)\n",
        "    parser.add_argument('-k', '--kl_targ', type=float, help='D_KL target value',\n",
        "                        default=0.003)\n",
        "    parser.add_argument('-b', '--batch_size', type=int,\n",
        "                        help='Number of episodes per training batch',\n",
        "                        default=20)\n",
        "    parser.add_argument('-m', '--hid1_mult', type=int,\n",
        "                        help='Size of first hidden layer for value and policy NNs'\n",
        "                             '(integer multiplier of observation dimension)',\n",
        "                        default=10)\n",
        "    parser.add_argument('-v', '--init_logvar', type=float,\n",
        "                        help='Initial policy log-variance (natural log of variance)',\n",
        "                        default=-1.0)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main(**vars(args))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhVJLiTkrY5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}