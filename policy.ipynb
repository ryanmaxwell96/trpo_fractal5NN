{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPggnR2j2cYFeDsqhZzUVKe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryanmaxwell96/trpo_fractal5NN/blob/master/policy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfWGxSm9mroG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "NN Policy with KL Divergence Constraint\n",
        "\n",
        "Written by Patrick Coady (pat-coady.github.io)\n",
        "\"\"\"\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "class Policy(object):\n",
        "    def __init__(self, obs_dim, act_dim, kl_targ, hid1_mult, init_logvar):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            obs_dim: num observation dimensions (int)\n",
        "            act_dim: num action dimensions (int)\n",
        "            kl_targ: target KL divergence between pi_old and pi_new\n",
        "            hid1_mult: size of first hidden layer, multiplier of obs_dim\n",
        "            init_logvar: natural log of initial policy variance\n",
        "        \"\"\"\n",
        "        self.beta = 1.0  # dynamically adjusted D_KL loss multiplier\n",
        "        eta = 50  # multiplier for D_KL-kl_targ hinge-squared loss\n",
        "        self.kl_targ = kl_targ\n",
        "        self.epochs = 20\n",
        "        self.lr_multiplier = 1.0  # dynamically adjust lr when D_KL out of control\n",
        "        self.trpo = TRPO(obs_dim, act_dim, hid1_mult, kl_targ, init_logvar, eta)\n",
        "        self.policy = self.trpo.get_layer('policy_nn')\n",
        "        self.lr = self.policy.get_lr()  # lr calculated based on size of PolicyNN\n",
        "        self.trpo.compile(optimizer=Adam(self.lr * self.lr_multiplier))\n",
        "        self.logprob_calc = LogProb()\n",
        "\n",
        "    def get_trpo_policy_model(self):\n",
        "        return self.trpo\n",
        "\n",
        "    def sample(self, obs):\n",
        "        \"\"\"Draw sample from policy.\"\"\"\n",
        "        act_means, act_logvars = self.policy(obs) # call in PolicyNN class\n",
        "        act_stddevs = np.exp(act_logvars / 2)\n",
        "\n",
        "        return np.random.normal(act_means, act_stddevs).astype(np.float32)\n",
        "\n",
        "    def update(self, observes, actions, advantages, logger):\n",
        "        \"\"\" Update policy based on observations, actions and advantages\n",
        "\n",
        "        Args:\n",
        "            observes: observations, shape = (N, obs_dim)\n",
        "            actions: actions, shape = (N, act_dim)\n",
        "            advantages: advantages, shape = (N,)\n",
        "            logger: Logger object, see utils.py\n",
        "        \"\"\"\n",
        "        K.set_value(self.trpo.optimizer.lr, self.lr * self.lr_multiplier)\n",
        "        K.set_value(self.trpo.beta, self.beta)\n",
        "        old_means, old_logvars = self.policy(observes) # call in PolicyNN class\n",
        "        old_means = old_means.numpy()\n",
        "        old_logvars = old_logvars.numpy()\n",
        "        old_logp = self.logprob_calc([actions, old_means, old_logvars]) # call in LogProb class\n",
        "        old_logp = old_logp.numpy()\n",
        "        loss, kl, entropy = 0, 0, 0\n",
        "        for e in range(self.epochs):\n",
        "            # print('observes')\n",
        "            # print(observes)\n",
        "            # print('actions')\n",
        "            # print(actions)\n",
        "            # print('advantages')\n",
        "            # print(advantages)\n",
        "            # print('old_means')\n",
        "            # print(old_means)\n",
        "            # print('old_logvars')\n",
        "            # print(old_logvars)\n",
        "            # print('old_logp')\n",
        "            # print(old_logp)\n",
        "            # input('')\n",
        "            # die = die\n",
        "            loss = self.trpo.train_on_batch([observes, actions, advantages, \n",
        "                                             old_means, old_logvars, old_logp])\n",
        "            # print('setting weight [0][0][0] to 500')\n",
        "            # print('weights')\n",
        "            # print(self.trpo.get_weights())\n",
        "            # input('')\n",
        "            kl, entropy = self.trpo.predict_on_batch([observes, actions, advantages,\n",
        "                                                      old_means, old_logvars, old_logp])\n",
        "            # kl->distance from previous policy\n",
        "            # entropy->amount of information contained in the new policy\n",
        "            kl, entropy = np.mean(kl), np.mean(entropy)\n",
        "            if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly: policy changed too far\n",
        "                break\n",
        "        # TODO: too many \"magic numbers\" in next 8 lines of code, need to clean up\n",
        "        if kl > self.kl_targ * 2:  # servo beta to reach D_KL target\n",
        "            self.beta = np.minimum(35, 1.5 * self.beta)  # max clip beta\n",
        "            if self.beta > 30 and self.lr_multiplier > 0.1:\n",
        "                self.lr_multiplier /= 1.5\n",
        "        elif kl < self.kl_targ / 2:\n",
        "            self.beta = np.maximum(1 / 35, self.beta / 1.5)  # min clip beta\n",
        "            if self.beta < (1 / 30) and self.lr_multiplier < 10:\n",
        "                self.lr_multiplier *= 1.5\n",
        "\n",
        "        logger.log({'PolicyLoss': loss,\n",
        "                    'PolicyEntropy': entropy,\n",
        "                    'KL': kl,\n",
        "                    'Beta': self.beta,\n",
        "                    '_lr_multiplier': self.lr_multiplier})\n",
        "\n",
        "class PolicyNN(Layer):\n",
        "    \"\"\" Neural net for policy approximation function.\n",
        "\n",
        "    Policy parameterized by Gaussian means and variances. NN outputs mean\n",
        "     action based on observation. Trainable variables hold log-variances\n",
        "     for each action dimension (i.e. variances not determined by NN).\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_dim, act_dim, hid1_mult, init_logvar, **kwargs):\n",
        "        super(PolicyNN, self).__init__(**kwargs)\n",
        "        self.batch_sz = None\n",
        "        self.init_logvar = init_logvar\n",
        "        hid1_units = obs_dim * hid1_mult\n",
        "        hid3_units = act_dim * 40  # 10 empirically determined\n",
        "        hid2_units = int(np.sqrt(hid1_units * hid3_units))\n",
        "        self.lr = 9e-4 / np.sqrt(hid2_units)  # 9e-4 empirically determined\n",
        "        # heuristic to set learning rate based on NN size (tuned on 'Hopper-v1')\n",
        "        self.dense1 = Dense(hid1_units, activation='tanh', input_shape=(obs_dim,))\n",
        "        self.dense2 = Dense(hid2_units, activation='tanh', input_shape=(hid1_units,))\n",
        "        self.dense3 = Dense(hid3_units, activation='tanh', input_shape=(hid2_units,))\n",
        "        self.dense4 = Dense(act_dim, input_shape=(hid3_units,))\n",
        "        \n",
        "        # logvar_speed increases learning rate for log-variances.\n",
        "        # heuristic sets logvar_speed based on network size.\n",
        "        logvar_speed = (10 * hid3_units) // 48\n",
        "        self.logvars = self.add_weight(shape=(logvar_speed, act_dim),\n",
        "                                       trainable=True, initializer='zeros')\n",
        "        print('Policy Params -- h1: {}, h2: {}, h3: {}, lr: {:.3g}, logvar_speed: {}'\n",
        "              .format(hid1_units, hid2_units, hid3_units, self.lr, logvar_speed))\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.batch_sz = input_shape[0]\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        y = self.dense1(inputs)\n",
        "        y = self.dense2(y)\n",
        "        y = self.dense3(y)\n",
        "        means = self.dense4(y)\n",
        "        logvars = K.sum(self.logvars, axis=0, keepdims=True) + self.init_logvar\n",
        "        logvars = K.tile(logvars, (self.batch_sz, 1))\n",
        "\n",
        "        return [means, logvars]\n",
        "\n",
        "    def get_lr(self):\n",
        "        return self.lr\n",
        "\n",
        "\n",
        "class KLEntropy(Layer):\n",
        "    \"\"\"\n",
        "    Layer calculates:\n",
        "        1. KL divergence between old and new distributions\n",
        "        2. Entropy of present policy\n",
        "\n",
        "    https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Kullback.E2.80.93Leibler_divergence\n",
        "    https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Entropy\n",
        "    \"\"\"\n",
        "    def __init__(self, act_dim,**kwargs):\n",
        "        super(KLEntropy, self).__init__(**kwargs)\n",
        "        # print('Ant->8,CartPole->1, or Humanoid-17')\n",
        "        self.act_dim = act_dim\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        old_means, old_logvars, new_means, new_logvars = inputs\n",
        "        log_det_cov_old = K.sum(old_logvars, axis=-1, keepdims=True)\n",
        "        log_det_cov_new = K.sum(new_logvars, axis=-1, keepdims=True)\n",
        "        trace_old_new = K.sum(K.exp(old_logvars - new_logvars), axis=-1, keepdims=True)\n",
        "        kl = 0.5 * (log_det_cov_new - log_det_cov_old + trace_old_new +\n",
        "                    K.sum(K.square(new_means - old_means) /\n",
        "                          K.exp(new_logvars), axis=-1, keepdims=True) -\n",
        "                    np.float32(self.act_dim))\n",
        "        entropy = 0.5 * (np.float32(self.act_dim) * (np.log(2 * np.pi) + 1.0) +\n",
        "                         K.sum(new_logvars, axis=-1, keepdims=True))\n",
        "\n",
        "        return [kl, entropy]\n",
        "\n",
        "\n",
        "class LogProb(Layer):\n",
        "    \"\"\"Layer calculates log probabilities of a batch of actions.\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super(LogProb, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        actions, act_means, act_logvars = inputs\n",
        "        logp = -0.5 * K.sum(act_logvars, axis=-1, keepdims=True)\n",
        "        logp += -0.5 * K.sum(K.square(actions - act_means) / K.exp(act_logvars),\n",
        "                             axis=-1, keepdims=True)\n",
        "\n",
        "        return logp\n",
        "\n",
        "\n",
        "class TRPO(Model):\n",
        "    def __init__(self, obs_dim, act_dim, hid1_mult, kl_targ, init_logvar, eta, **kwargs):\n",
        "        super(TRPO, self).__init__(**kwargs)\n",
        "        self.kl_targ = kl_targ\n",
        "        self.eta = eta\n",
        "        \n",
        "        print('beta initializer')\n",
        "        beta = input('')\n",
        "        beta = float(beta)\n",
        "        kernel_init = initializers.Constant(beta)\n",
        "        self.beta = self.add_weight('beta', initializer=kernel_init, trainable=False)\n",
        "        self.policy = PolicyNN(obs_dim, act_dim, hid1_mult, init_logvar)\n",
        "        self.logprob = LogProb()\n",
        "        self.kl_entropy = KLEntropy(act_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        obs, act, adv, old_means, old_logvars, old_logp = inputs\n",
        "        new_means, new_logvars = self.policy(obs) # call in PolicyNN class\n",
        "        new_logp = self.logprob([act, new_means, new_logvars]) # call in LogProb class \n",
        "        kl, entropy = self.kl_entropy([old_means, old_logvars, # call in KLEntropy class\n",
        "                                       new_means, new_logvars])\n",
        "        loss1 = -K.mean(adv * K.exp(new_logp - old_logp))\n",
        "        loss2 = K.mean(self.beta * kl)\n",
        "        # TODO - Take mean before or after hinge loss?\n",
        "        loss3 = self.eta * K.square(K.maximum(0.0, K.mean(kl) - 2.0 * self.kl_targ))\n",
        "        self.add_loss(loss1 + loss2 + loss3)\n",
        "\n",
        "        return [kl, entropy]\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}